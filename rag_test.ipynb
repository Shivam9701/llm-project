{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG Pipeline Creation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this step, we will create a database to store the research papers for a given user query. To do this, we first need to retrieve a list of relevant papers from the arXiv API for the query.\n",
    "\n",
    "- We will be using the ArxivLoader() package from LangChain as it abstracts API interactions, and retrieves the papers for further processing. \n",
    "\n",
    "- We can split these papers into smaller chunks to ensure efficient processing and relevant information retrieval later on. To do this, we will use the RecursiveTextSplitter() from LangChain, since it ensures semantic preservation of information while splitting documents. \n",
    "\n",
    "- Next, we will create embeddings for these chunks using the sentence-transformers embeddings from HuggingFace. Finally, we will ingest these split document embeddings into a Chroma DB database for further querying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.llms import Replicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"lightweight transformers for textual data\"\n",
    "arxiv_docs = ArxivLoader(query=query, load_max_docs=3).load()\n",
    "pdf_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in arxiv_docs:\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=100,\n",
    "    )\n",
    "    texts = text_splitter.create_documents([doc.page_content])\n",
    "    pdf_data.append(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shiva\\OneDrive\\Desktop\\Data Science\\Project\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-l6-v2\")\n",
    "db = Chroma.from_documents(pdf_data[0], embedding=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval and Generation\n",
    "\n",
    "- Once the database for a particular topic has been created, we can use this database as a retriever to answer user questions based on the provided context. LangChain offers a few different chains for retrieval, the simplest being the RetrievalQA chain that we will use. We will set it up using the from_chain_type() method, specifying the model and the retriever. For document integration into the LLM, weâ€™ll use the stuff chain type, as it stuffs all documents into a single prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Replicate(\n",
    "    model=\"meta/meta-llama-3-70b-instruct\",\n",
    "    model_kwargs={\"temperature\": 0.0, \"top_p\": 1, \"max_new_tokens\": 1000},\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, chain_type=\"stuff\", retriever=db.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Overview of language models, their applications, and their capabilities?',\n",
       " 'result': 'Based on the provided context, here is an overview of language models, their applications, and their capabilities:\\n\\n**Overview of Language Models:**\\nLanguage models, such as ChatGPT and GPT-4, are pre-trained using self-supervised learning, which leverages the data itself as supervision. They can capture different levels of language representation, such as words, sentences, or documents. The process of training a language model involves data preparation, pre-training, and fine-tuning.\\n\\n**Applications:**\\nLanguage models have various applications, including:\\n\\n* Natural Language Processing (NLP) tasks, such as text summarization, question answering, and language translation\\n* Multi-player games, such as Poker\\n* Visual reasoning and coordination\\n* Medium-range global weather forecasting\\n* Multi-modal modeling with in-context instruction tuning\\n\\n**Capabilities:**\\nLanguage models have the capability to:\\n\\n* Predict the next word in a sentence, with a lower perplexity indicating better performance\\n* Adapt to different NLP tasks through fine-tuning\\n* Capture different levels of language representation\\n* Enhance their performance and applicability through system design and end-to-end training\\n* Be used as a dispatcher to orchestrate different expert models for specific tasks\\n\\nNote that this overview is based on the provided context and may not be a comprehensive summary of language models, their applications, and their capabilities.'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Overview of language models, their applications, and their capabilities?\"\n",
    "answer = qa({\"query\": question})\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_community.reporters'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Create a Markdown report for the answer['result']\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreporters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MarkdownReporter\n\u001b[0;32m      5\u001b[0m reporter \u001b[38;5;241m=\u001b[39m MarkdownReporter()\n\u001b[0;32m      6\u001b[0m reporter\u001b[38;5;241m.\u001b[39mcreate_report(answer[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'langchain_community.reporters'"
     ]
    }
   ],
   "source": [
    "# Create a Markdown report for the answer['result']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
